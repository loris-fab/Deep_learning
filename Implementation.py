from pydub import AudioSegment
import noisereduce as nr
import librosa
import soundfile as sf
import torch
import torchaudio
from silero_vad import get_speech_timestamps, collect_chunks
import io
import numpy as np
from pyannote.audio.pipelines import SpeakerDiarization
from pyannote.core import Segment
from tqdm import tqdm
import whisper
import Implementation as imp
import pandas as pd
import subprocess
import sys
import os


import torch

# ğŸ“Œ Chargement du modÃ¨le Silero VAD
model_and_utils = torch.hub.load(
    repo_or_dir='snakers4/silero-vad',
    model='silero_vad',
    force_reload=True,
    trust_repo=True  # Ã‰vite l'avertissement "untrusted repository"
)

# ğŸ“Œ Extraction correcte des Ã©lÃ©ments du tuple
model = model_and_utils[0]  # Le modÃ¨le PyTorch
utils_tuple = model_and_utils[1]  # Tuple contenant les fonctions utilitaires

# ğŸ“Œ Assignation des fonctions utiles
get_speech_timestamps = utils_tuple[0]  # Fonction de dÃ©tection des segments parlÃ©s
save_audio = utils_tuple[1]  # Fonction de sauvegarde audio (optionnelle)
read_audio = utils_tuple[2]  # Fonction de lecture de l'audio
VADIterator = utils_tuple[3]  # Classe pour gÃ©rer le VAD
collect_chunks = utils_tuple[4]  # Fonction pour extraire les morceaux de speech

# VÃ©rification
#print(f"âœ… get_speech_timestamps rÃ©cupÃ©rÃ© : {get_speech_timestamps}")
#print(f"âœ… collect_chunks rÃ©cupÃ©rÃ© : {collect_chunks}")


# FONCTION D'EXTRACTION DE L'AUDIO

def extract_audio(video_path, output_audio_path):
    '''
    Explication des options ffmpeg
    -ac 1 â†’ Convertit lâ€™audio en mono
    -ar 16000 â†’ DÃ©finit la frÃ©quence dâ€™Ã©chantillonnage Ã  16 kHz (utile pour certaines applications)
    -q:a 0 â†’ QualitÃ© audio maximale
    -map a â†’ Extrait uniquement la piste audio
    -vn â†’ DÃ©sactive la vidÃ©o
    '''
    command = f'ffmpeg -i "{video_path}" -vn -ac 1 -ar 16000 -q:a 0 -map a "{output_audio_path}"'
    subprocess.run(command, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

    if os.path.exists(output_audio_path):
        print(f"âœ… Audio extrait avec succÃ¨s : {output_audio_path}")
    else:
        print(f"âŒ Ã‰chec de l'extraction audio pour : {video_path}")


def extract_all_audio(Video_folder, Audio_folder):
    print("##########################################")
    for video in os.listdir(Video_folder):
        if video.endswith(".mp4"):
            video_path = os.path.join(Video_folder, video)
            audio_path = os.path.join(Audio_folder, video.replace(".mp4", ".wav"))
            extract_audio(video_path , audio_path)
    print("Extraction de l'audio terminÃ©e !")
    print("##########################################")


def time_to_seconds(time_str):
    """Convertit une chaÃ®ne de temps HH:MM:SS en secondes."""
    h, m, s = map(int, time_str.split(":"))
    return h * 3600 + m * 60 + s

def extract_snippets(audio_path, output_path, snippets):
    """
    Extrait et concatÃ¨ne des parties spÃ©cifiques d'un fichier audio.

    :param audio_path: Chemin du fichier audio d'entrÃ©e
    :param output_path: Chemin du fichier audio de sortie
    :param snippets: Liste de listes [["HH:MM:SS", "HH:MM:SS"]]
    """
    audio = AudioSegment.from_file(audio_path)
    extracted_audio = AudioSegment.empty()  # Audio final vide au dÃ©part

    for start, end in snippets:
        start_sec = time_to_seconds(start)
        end_sec = time_to_seconds(end)
        extracted_audio += audio[start_sec * 1000:end_sec * 1000]  # Convertir en millisecondes

    # Sauvegarde du fichier final
    extracted_audio.export(output_path, format="wav")


# PREPROCESSING AUDIOS

def format_time(seconds):
    """Convertit un temps en secondes vers HH:MM:SS"""
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    return f"{h:02}:{m:02}:{s:02}"

def reduce_noise(audio, sr):
    """
    Applique une rÃ©duction de bruit sur l'audio.
    """
    return nr.reduce_noise(y=audio, sr=sr)

def save_audio(audio, sr, output_path):
    """
    Sauvegarde un fichier audio au format WAV.
    """
    sf.write(output_path, audio, sr)

def detect_music_and_voice(audio, sr):
    """
    DÃ©tecte si l'audio contient de la musique et identifie si une voix est prÃ©sente avec.
    Utilise MFCCs, Zero Crossing Rate (ZCR) et analyse spectrale pour diffÃ©rencier :
    - Musique seule
    - Voix seule
    - Voix + Musique
    """

    # ğŸ”¹ Analyse des MFCCs (signature musique vs voix)
    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
    mfcc_var = np.var(mfcc, axis=1)

    # ğŸ”¹ Calcul du Zero Crossing Rate (ZCR) â†’ DÃ©tecte les transitions rapides dans la musique
    zcr = librosa.feature.zero_crossing_rate(y=audio)
    avg_zcr = np.mean(zcr)

    # ğŸ”¹ Analyse du spectrogramme â†’ Formants vocaux
    spec = librosa.amplitude_to_db(librosa.stft(audio), ref=np.max)
    vocal_energy = np.mean(spec[50:300, :])  # 50Hz-300Hz = frÃ©quences vocales

    # ğŸ”¹ DÃ©tection de musique pure vs voix
    is_music = np.mean(mfcc_var) < 50 and avg_zcr > 0.05
    is_voice = vocal_energy > -20  # Plus de -20dB dans les frÃ©quences vocales = voix prÃ©sente

    if is_music and is_voice:
        return "Voix + Musique"
    elif is_music:
        return "Musique seule"
    elif is_voice:
        return "Voix seule"
    else:
        return "Silence"

def preprocess_audio(input_path, output_path):
    """
    Nettoie l'audio et conserve la mÃªme durÃ©e en remplaÃ§ant les silences par du silence audio.
    
    - input_path : Chemin du fichier audio en entrÃ©e (16 kHz, mono)
    - output_path : Chemin du fichier nettoyÃ©
    - vad_threshold : SensibilitÃ© du VAD (0.3 = sensible, 0.5 = normal, 0.7 = strict)
    
    Retourne :
    - Un fichier audio nettoyÃ© avec la mÃªme durÃ©e
    - Une liste des timestamps des parties parlÃ©es
    """

    # ğŸ”¹ 1. Chargement de l'audio /music ...
    audio, sr = librosa.load(input_path, sr=16000)  # Assure un Ã©chantillonnage Ã  16kHz
    original_duration = len(audio)  # Nombre d'Ã©chantillons

    """
    # DÃ©tection de musique et voix
    category = detect_music_and_voice(audio, sr)
    if category == "Voix + Musique":
        threshold = 0.4  # ğŸµ Voix dans la musique â†’ Capture bien la parole
    elif category == "Musique seule":
        threshold = 0.8  # ğŸµ Musique seule â†’ Ignorer
    elif category == "Voix seule":
        threshold = 0.3  # ğŸ™ï¸ Seulement Voix â†’ Capturer toute la parole
    else:
        threshold = 0.7  # Silence ou bruit â†’ Ignorer
    """
    threshold = 0.4

    # ğŸ”¹ 2. RÃ©duction du bruit
    audio = nr.reduce_noise(y=audio, sr=sr)

    # ğŸ”¹ 3. DÃ©tection des segments parlÃ©s
    speech_timestamps = get_speech_timestamps(audio, model,sampling_rate=sr, threshold=threshold)

    # ğŸ”¹ 4. CrÃ©ation d'un nouvel audio avec silences Ã  la place des blancs
    cleaned_audio = np.zeros(original_duration, dtype=np.float32)  # Commence par du silence total

    speech_ranges = []
    for seg in speech_timestamps:
        start_sample, end_sample = seg['start'], seg['end']
        cleaned_audio[start_sample:end_sample] = audio[start_sample:end_sample]  # Remet les parties parlÃ©es
        speech_ranges.append([format_time(start_sample / sr), format_time(end_sample / sr)])  # Sauvegarde timestamps

    # ğŸ”¹ 5. Sauvegarde de l'audio nettoyÃ© avec silences
    sf.write(output_path, cleaned_audio, sr)

    print(f"âœ… Audio nettoyÃ© et sauvegardÃ© : {output_path}")
    #print(f"ğŸµ CatÃ©gorie dÃ©tectÃ©e : {category} â†’ Threshold = {threshold}")
    #print(f"ğŸ™ï¸ Segments parlÃ©s dÃ©tectÃ©s : {speech_ranges}")

    return speech_ranges


import os
import pandas as pd

def preprocess_all_audio(audio_path, output_audio_clean_path):
    data = []
    
    for i, audio_file in enumerate(os.listdir(audio_path)):
        if audio_file.endswith(".wav"):
            input_audio_path = os.path.join(audio_path, audio_file)
            output_clean_path = os.path.join(output_audio_clean_path, audio_file)
            
            speech_ranges = preprocess_audio(input_audio_path, output_clean_path)
            data.append({"audio_name": audio_file, "speech_ranges": speech_ranges})
    df = pd.DataFrame(data)

    if data:  # VÃ©rifie si au moins un fichier a Ã©tÃ© traitÃ©
        print(f"âœ… {len(data)} fichiers audio nettoyÃ©s avec succÃ¨s.")
    else:
        print(f"âŒ Aucun fichier audio n'a Ã©tÃ© traitÃ©.")

    return df
