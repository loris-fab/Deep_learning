from pydub import AudioSegment
import noisereduce as nr
import librosa
import soundfile as sf
import torch
import torchaudio
from silero_vad import get_speech_timestamps, collect_chunks
import io
import numpy as np
from pyannote.audio.pipelines import SpeakerDiarization
from pyannote.core import Segment
from tqdm import tqdm
import whisper
import Implementation as imp
import pandas as pd
import subprocess
import sys
import os



# üìå Chargement du mod√®le Silero VAD
model_and_utils = torch.hub.load(
    repo_or_dir='snakers4/silero-vad',
    model='silero_vad',
    force_reload=True,
    trust_repo=True  # √âvite l'avertissement "untrusted repository"
)

# üìå Extraction correcte des √©l√©ments du tuple
model = model_and_utils[0]  # Le mod√®le PyTorch
utils_tuple = model_and_utils[1]  # Tuple contenant les fonctions utilitaires

# üìå Assignation des fonctions utiles
get_speech_timestamps = utils_tuple[0]  # Fonction de d√©tection des segments parl√©s
save_audio = utils_tuple[1]  # Fonction de sauvegarde audio (optionnelle)
read_audio = utils_tuple[2]  # Fonction de lecture de l'audio
VADIterator = utils_tuple[3]  # Classe pour g√©rer le VAD
collect_chunks = utils_tuple[4]  # Fonction pour extraire les morceaux de speech

# V√©rification
#print(f"‚úÖ get_speech_timestamps r√©cup√©r√© : {get_speech_timestamps}")
#print(f"‚úÖ collect_chunks r√©cup√©r√© : {collect_chunks}")


# FONCTION D'EXTRACTION DE L'AUDIO

def extract_audio(video_path, output_audio_path):
    '''
    Explication des options ffmpeg
    -ac 1 ‚Üí Convertit l‚Äôaudio en mono
    -ar 16000 ‚Üí D√©finit la fr√©quence d‚Äô√©chantillonnage √† 16 kHz (utile pour certaines applications)
    -q:a 0 ‚Üí Qualit√© audio maximale
    -map a ‚Üí Extrait uniquement la piste audio
    -vn ‚Üí D√©sactive la vid√©o
    '''
    command = f'ffmpeg -i "{video_path}" -vn -ac 1 -ar 16000 -q:a 0 -map a "{output_audio_path}"'
    subprocess.run(command, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

    if os.path.exists(output_audio_path):
        print(f"‚úÖ Audio extrait avec succ√®s : {output_audio_path}")
    else:
        print(f"‚ùå √âchec de l'extraction audio pour : {video_path}")


def extract_all_audio(Video_folder, Audio_folder):
    print("##########################################")
    for video in os.listdir(Video_folder):
        if video.endswith(".mp4"):
            video_path = os.path.join(Video_folder, video)
            audio_path = os.path.join(Audio_folder, video.replace(".mp4", ".wav"))
            extract_audio(video_path , audio_path)
    print("Extraction de l'audio termin√©e !")
    print("##########################################")


def time_to_seconds(time_str):
    """Convertit une cha√Æne de temps HH:MM:SS en secondes."""
    h, m, s = map(int, time_str.split(":"))
    return h * 3600 + m * 60 + s

def extract_snippets(audio_path, output_path, snippets):
    """
    Extrait et concat√®ne des parties sp√©cifiques d'un fichier audio.

    :param audio_path: Chemin du fichier audio d'entr√©e
    :param output_path: Chemin du fichier audio de sortie
    :param snippets: Liste de listes [["HH:MM:SS", "HH:MM:SS"]]
    """
    audio = AudioSegment.from_file(audio_path)
    extracted_audio = AudioSegment.empty()  # Audio final vide au d√©part

    for start, end in snippets:
        start_sec = time_to_seconds(start)
        end_sec = time_to_seconds(end)
        extracted_audio += audio[start_sec * 1000:end_sec * 1000]  # Convertir en millisecondes

    # Sauvegarde du fichier final
    extracted_audio.export(output_path, format="wav")


# PREPROCESSING AUDIOS

def format_time(seconds):
    """Convertit un temps en secondes vers HH:MM:SS"""
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    return f"{h:02}:{m:02}:{s:02}"

def reduce_noise(audio, sr):
    """
    Applique une r√©duction de bruit sur l'audio.
    """
    return nr.reduce_noise(y=audio, sr=sr)

def save_audio(audio, sr, output_path):
    """
    Sauvegarde un fichier audio au format WAV.
    """
    sf.write(output_path, audio, sr)

def detect_music_and_voice(audio, sr):
    """
    D√©tecte si l'audio contient de la musique et identifie si une voix est pr√©sente avec.
    Utilise MFCCs, Zero Crossing Rate (ZCR) et analyse spectrale pour diff√©rencier :
    - Musique seule
    - Voix seule
    - Voix + Musique
    """

    # üîπ Analyse des MFCCs (signature musique vs voix)
    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
    mfcc_var = np.var(mfcc, axis=1)

    # üîπ Calcul du Zero Crossing Rate (ZCR) ‚Üí D√©tecte les transitions rapides dans la musique
    zcr = librosa.feature.zero_crossing_rate(y=audio)
    avg_zcr = np.mean(zcr)

    # üîπ Analyse du spectrogramme ‚Üí Formants vocaux
    spec = librosa.amplitude_to_db(librosa.stft(audio), ref=np.max)
    vocal_energy = np.mean(spec[50:300, :])  # 50Hz-300Hz = fr√©quences vocales

    # üîπ D√©tection de musique pure vs voix
    is_music = np.mean(mfcc_var) < 50 and avg_zcr > 0.05
    is_voice = vocal_energy > -20  # Plus de -20dB dans les fr√©quences vocales = voix pr√©sente

    if is_music and is_voice:
        return "Voix + Musique"
    elif is_music:
        return "Musique seule"
    elif is_voice:
        return "Voix seule"
    else:
        return "Silence"

def preprocess_audio(input_path, output_path):
    """
    Nettoie l'audio et conserve la m√™me dur√©e en rempla√ßant les silences par du silence audio.
    
    - input_path : Chemin du fichier audio en entr√©e (16 kHz, mono)
    - output_path : Chemin du fichier nettoy√©
    - vad_threshold : Sensibilit√© du VAD (0.3 = sensible, 0.5 = normal, 0.7 = strict)
    
    Retourne :
    - Un fichier audio nettoy√© avec la m√™me dur√©e
    - Une liste des timestamps des parties parl√©es
    """

    # üîπ 1. Chargement de l'audio /music ...
    audio, sr = librosa.load(input_path, sr=16000)  # Assure un √©chantillonnage √† 16kHz
    original_duration = len(audio)  # Nombre d'√©chantillons

    """
    # D√©tection de musique et voix
    category = detect_music_and_voice(audio, sr)
    if category == "Voix + Musique":
        threshold = 0.4  # üéµ Voix dans la musique ‚Üí Capture bien la parole
    elif category == "Musique seule":
        threshold = 0.8  # üéµ Musique seule ‚Üí Ignorer
    elif category == "Voix seule":
        threshold = 0.3  # üéôÔ∏è Seulement Voix ‚Üí Capturer toute la parole
    else:
        threshold = 0.7  # Silence ou bruit ‚Üí Ignorer
    """
    threshold = 0.4

    # üîπ 2. R√©duction du bruit
    audio = nr.reduce_noise(y=audio, sr=sr)

    # üîπ 3. D√©tection des segments parl√©s
    speech_timestamps = get_speech_timestamps(audio, model,sampling_rate=sr, threshold=threshold)

    # üîπ 4. Cr√©ation d'un nouvel audio avec silences √† la place des blancs
    cleaned_audio = np.zeros(original_duration, dtype=np.float32)  # Commence par du silence total

    speech_ranges = []
    for seg in speech_timestamps:
        start_sample, end_sample = seg['start'], seg['end']
        cleaned_audio[start_sample:end_sample] = audio[start_sample:end_sample]  # Remet les parties parl√©es
        speech_ranges.append([format_time(start_sample / sr), format_time(end_sample / sr)])  # Sauvegarde timestamps

    # üîπ 5. Sauvegarde de l'audio nettoy√© avec silences
    sf.write(output_path, cleaned_audio, sr)

    print(f"‚úÖ Audio nettoy√© et sauvegard√© : {output_path}")
    #print(f"üéµ Cat√©gorie d√©tect√©e : {category} ‚Üí Threshold = {threshold}")
    #print(f"üéôÔ∏è Segments parl√©s d√©tect√©s : {speech_ranges}")

    return speech_ranges


import os
import pandas as pd

def preprocess_all_audio(audio_path, output_audio_clean_path):
    data = []
    
    for i, audio_file in enumerate(os.listdir(audio_path)):
        if audio_file.endswith(".wav"):
            input_audio_path = os.path.join(audio_path, audio_file)
            output_clean_path = os.path.join(output_audio_clean_path, audio_file)
            
            speech_ranges = preprocess_audio(input_audio_path, output_clean_path)
            data.append({"audio_name": audio_file, "speech_ranges": speech_ranges})
    df = pd.DataFrame(data)

    if data:  # V√©rifie si au moins un fichier a √©t√© trait√©
        print(f"‚úÖ {len(data)} fichiers audio nettoy√©s avec succ√®s.")
    else:
        print(f"‚ùå Aucun fichier audio n'a √©t√© trait√©.")

    return df


# SECOND FILTER : FILTER TEXT IN IMAGES WITH OCR AND NLP


def seconds_to_hhmmss(seconds):
    return str(timedelta(seconds=int(seconds)))

def approximate_hate_speech_from_text_from_image(
    video_path: str,
    sampling_time_froid: float,
    sampling_time_chaud: float,
    time_to_recover: float,
    merge_final_snippet_time: float,
    detect_hate_speech_in_image
):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError("Could not open video file")

    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps

    current_time = 0.0
    state = "froid"
    time_in_chaud = 0.0
    hate_timestamps = []

    while current_time < duration:
        cap.set(cv2.CAP_PROP_POS_MSEC, current_time * 1000)
        ret, frame = cap.read()
        if not ret:
            break

        temp_image_path = "/tmp/temp_frame.jpg"
        cv2.imwrite(temp_image_path, frame)

        result = detect_hate_speech_in_image(temp_image_path)
        os.remove(temp_image_path)

        if result.get("hate_detected", False):
            hate_timestamps.append(current_time)
            state = "chaud"
            time_in_chaud = 0.0
        elif state == "chaud":
            time_in_chaud += sampling_time_chaud
            if time_in_chaud >= time_to_recover:
                state = "froid"

        current_time += sampling_time_chaud if state == "chaud" else sampling_time_froid

    cap.release()

    # Fusion of the time intervals
    intervals = [(max(0, t - merge_final_snippet_time), min(duration, t + merge_final_snippet_time)) for t in hate_timestamps]
    merged_intervals = []
    for start, end in sorted(intervals):
        if not merged_intervals or start > merged_intervals[-1][1]:
            merged_intervals.append([start, end])
        else:
            merged_intervals[-1][1] = max(merged_intervals[-1][1], end)

    # Formater les intervalles
    formatted_intervals = [[seconds_to_hhmmss(start), seconds_to_hhmmss(end)] for start, end in merged_intervals]

    return formatted_intervals


reader = easyocr.Reader(['en'])  # detects the language of the text
nlp_classifier = pipeline("text-classification", model="Hate-speech-CNERG/dehatebert-mono-english")

def detect_hate_speech_in_image(image_path):
    # üñºÔ∏è OCR
    text_blocks = reader.readtext(image_path, detail=0)
    full_text = " ".join(text_blocks).strip()

    if not full_text:
        return {
            "text": None,
            "hate_detected": False,
            "score": 0.0,
            "reason": "No text detected"
        }

    # üß† NLP (classification hate speech)
    prediction = nlp_classifier(full_text)[0]

    return {
        "text": full_text,
        "hate_detected": prediction['label'].lower() == 'hate',
        "score": float(prediction['score']),
        "reason": prediction['label']
    }
