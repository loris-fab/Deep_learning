from pydub import AudioSegment
import noisereduce as nr
import librosa
import soundfile as sf
import torchaudio
from silero_vad import get_speech_timestamps, collect_chunks
import io
import numpy as np
from pyannote.audio.pipelines import SpeakerDiarization
from pyannote.core import Segment
from tqdm import tqdm
#import whisper
import pandas as pd
import subprocess
import sys
import os
import torch
import torch.nn as nn
import cv2
from datetime import timedelta
import easyocr
from transformers import pipeline
from codecarbon import EmissionsTracker
from transformers import CLIPProcessor, CLIPModel, AutoTokenizer, BertModel
from PIL import Image
import re




# üìå Chargement du mod√®le Silero VAD
model_and_utils = torch.hub.load(
    repo_or_dir='snakers4/silero-vad',
    model='silero_vad',
    force_reload=True,
    trust_repo=True  # √âvite l'avertissement "untrusted repository"
)

# üìå Extraction correcte des √©l√©ments du tuple
model = model_and_utils[0]  # Le mod√®le PyTorch
utils_tuple = model_and_utils[1]  # Tuple contenant les fonctions utilitaires

# üìå Assignation des fonctions utiles
get_speech_timestamps = utils_tuple[0]  # Fonction de d√©tection des segments parl√©s
save_audio = utils_tuple[1]  # Fonction de sauvegarde audio (optionnelle)
read_audio = utils_tuple[2]  # Fonction de lecture de l'audio
VADIterator = utils_tuple[3]  # Classe pour g√©rer le VAD
collect_chunks = utils_tuple[4]  # Fonction pour extraire les morceaux de speech

# V√©rification
#print(f"‚úÖ get_speech_timestamps r√©cup√©r√© : {get_speech_timestamps}")
#print(f"‚úÖ collect_chunks r√©cup√©r√© : {collect_chunks}")


# FONCTION D'EXTRACTION DE L'AUDIO

def extract_audio(video_path, output_audio_path):
    '''
    Explication des options ffmpeg
    -ac 1 ‚Üí Convertit l‚Äôaudio en mono
    -ar 16000 ‚Üí D√©finit la fr√©quence d‚Äô√©chantillonnage √† 16 kHz (utile pour certaines applications)
    -q:a 0 ‚Üí Qualit√© audio maximale
    -map a ‚Üí Extrait uniquement la piste audio
    -vn ‚Üí D√©sactive la vid√©o
    '''
    command = f'ffmpeg -i "{video_path}" -vn -ac 1 -ar 16000 -q:a 0 -map a "{output_audio_path}"'
    subprocess.run(command, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

    if os.path.exists(output_audio_path):
        print(f"‚úÖ Audio extracted successfully : {output_audio_path}") 
    else:
        print(f"‚ùå Echec of audio extraction : {video_path}") 


def extract_all_audio(Video_folder, Audio_folder):
    print("##########################################")
    for video in os.listdir(Video_folder):
        if video.endswith(".mp4"):
            video_path = os.path.join(Video_folder, video)
            audio_path = os.path.join(Audio_folder, video.replace(".mp4", ".wav"))
            extract_audio(video_path , audio_path)
    print("Extraction de l'audio termin√©e !")
    print("##########################################")


def time_to_seconds(time_str):
    """Convertit une cha√Æne de temps HH:MM:SS en secondes."""
    h, m, s = map(int, time_str.split(":"))
    return h * 3600 + m * 60 + s

def extract_snippets(audio_path, output_path, snippets):
    """
    Extrait et concat√®ne des parties sp√©cifiques d'un fichier audio.

    :param audio_path: Chemin du fichier audio d'entr√©e
    :param output_path: Chemin du fichier audio de sortie
    :param snippets: Liste de listes [["HH:MM:SS", "HH:MM:SS"]]
    """
    audio = AudioSegment.from_file(audio_path)
    extracted_audio = AudioSegment.empty()  # Audio final vide au d√©part

    for start, end in snippets:
        start_sec = time_to_seconds(start)
        end_sec = time_to_seconds(end)
        extracted_audio += audio[start_sec * 1000:end_sec * 1000]  # Convertir en millisecondes

    # Sauvegarde du fichier final
    extracted_audio.export(output_path, format="wav")


# PREPROCESSING AUDIOS

def format_time(seconds):
    """Convertit un temps en secondes vers HH:MM:SS"""
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    return f"{h:02}:{m:02}:{s:02}"

def reduce_noise(audio, sr):
    """
    Applique une r√©duction de bruit sur l'audio.
    """
    return nr.reduce_noise(y=audio, sr=sr)

def save_audio(audio, sr, output_path):
    """
    Sauvegarde un fichier audio au format WAV.
    """
    sf.write(output_path, audio, sr)

def detect_music_and_voice(audio, sr):
    """
    D√©tecte si l'audio contient de la musique et identifie si une voix est pr√©sente avec.
    Utilise MFCCs, Zero Crossing Rate (ZCR) et analyse spectrale pour diff√©rencier :
    - Musique seule
    - Voix seule
    - Voix + Musique
    """

    # üîπ Analyse des MFCCs (signature musique vs voix)
    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
    mfcc_var = np.var(mfcc, axis=1)

    # üîπ Calcul du Zero Crossing Rate (ZCR) ‚Üí D√©tecte les transitions rapides dans la musique
    zcr = librosa.feature.zero_crossing_rate(y=audio)
    avg_zcr = np.mean(zcr)

    # üîπ Analyse du spectrogramme ‚Üí Formants vocaux
    spec = librosa.amplitude_to_db(librosa.stft(audio), ref=np.max)
    vocal_energy = np.mean(spec[50:300, :])  # 50Hz-300Hz = fr√©quences vocales

    # üîπ D√©tection de musique pure vs voix
    is_music = np.mean(mfcc_var) < 50 and avg_zcr > 0.05
    is_voice = vocal_energy > -20  # Plus de -20dB dans les fr√©quences vocales = voix pr√©sente

    if is_music and is_voice:
        return "Voix + Musique"
    elif is_music:
        return "Musique seule"
    elif is_voice:
        return "Voix seule"
    else:
        return "Silence"

def expand_and_merge_speech_timestamps(speech_timestamps, sr=16000, margin=1.5):
    """
    √âlargit chaque segment parl√© de ¬±margin (en secondes), puis fusionne les chevauchements.
    Fonctionne directement sur les √©chantillons.
    """
    # √âtape 1 : √©largir
    expanded = []
    margin_samples = int(margin * sr)
    for seg in speech_timestamps:
        start = max(seg['start'] - margin_samples, 0)
        end = seg['end'] + margin_samples
        expanded.append([start, end])

    # √âtape 2 : fusionner
    expanded.sort()
    merged = []
    for seg in expanded:
        if not merged or seg[0] > merged[-1][1]:
            merged.append(seg)
        else:
            merged[-1][1] = max(merged[-1][1], seg[1])

    # √âtape 3 : retransformer en format [{'start': x, 'end': y}]
    return [{'start': start, 'end': end} for start, end in merged]

def preprocess_audio(input_path, output_path, threshold_CDA = 0.2):
    """
    Nettoie l'audio et conserve la m√™me dur√©e en rempla√ßant les silences par du silence audio.
    
    - input_path : Chemin du fichier audio en entr√©e (16 kHz, mono)
    - output_path : Chemin du fichier nettoy√©
    - vad_threshold : Sensibilit√© du VAD (0.3 = sensible, 0.5 = normal, 0.7 = strict)
    
    Retourne :
    - Un fichier audio nettoy√© avec la m√™me dur√©e
    - Une liste des timestamps des parties parl√©es
    """

    # üîπ 1. Chargement de l'audio /music ...
    audio, sr = librosa.load(input_path, sr=16000)  # Assure un √©chantillonnage √† 16kHz
    original_duration = len(audio)  # Nombre d'√©chantillons

    """
    # D√©tection de musique et voix
    category = detect_music_and_voice(audio, sr)
    if category == "Voix + Musique":
        threshold = 0.4  # üéµ Voix dans la musique ‚Üí Capture bien la parole
    elif category == "Musique seule":
        threshold = 0.8  # üéµ Musique seule ‚Üí Ignorer
    elif category == "Voix seule":
        threshold = 0.3  # üéôÔ∏è Seulement Voix ‚Üí Capturer toute la parole
    else:
        threshold = 0.7  # Silence ou bruit ‚Üí Ignorer
    """
    threshold = threshold_CDA

    # üîπ 2. R√©duction du bruit
    audio = nr.reduce_noise(y=audio, sr=sr)

    # üîπ 3. D√©tection des segments parl√©s
    speech_timestamps = get_speech_timestamps(audio, model,sampling_rate=sr, threshold=threshold)
    # üîπ 4. Cr√©ation d'un nouvel audio avec silences √† la place des blancs
    cleaned_audio = np.zeros(original_duration, dtype=np.float32)  # Commence par du silence total

    speech_ranges = []
    for seg in expand_and_merge_speech_timestamps(speech_timestamps):
        start_sample, end_sample = seg['start'], seg['end']
        cleaned_audio[start_sample:end_sample] = audio[start_sample:end_sample]  # Remet les parties parl√©es
        speech_ranges.append([format_time(start_sample / sr), format_time(end_sample / sr)])  # Sauvegarde timestamps

    # üîπ 5. Sauvegarde de l'audio nettoy√© avec silences
    sf.write(output_path, cleaned_audio, sr)

    print(f"‚úÖ Audio cleaned : {output_path}") 
    #print(f"üéµ Cat√©gorie d√©tect√©e : {category} ‚Üí Threshold = {threshold}")
    #print(f"üéôÔ∏è Segments parl√©s d√©tect√©s : {speech_ranges}")

    return speech_ranges


import os
import pandas as pd

def preprocess_all_audio(audio_path, output_audio_clean_path):
    data = []
    
    for i, audio_file in enumerate(os.listdir(audio_path)):
        if audio_file.endswith(".wav"):
            input_audio_path = os.path.join(audio_path, audio_file)
            output_clean_path = os.path.join(output_audio_clean_path, audio_file)
            
            speech_ranges = preprocess_audio(input_audio_path, output_clean_path)
            data.append({"audio_name": audio_file, "speech_ranges": speech_ranges})
    df = pd.DataFrame(data)

    if data:  # V√©rifie si au moins un fichier a √©t√© trait√©
        print(f"‚úÖ {len(data)} fichiers audio nettoy√©s avec succ√®s.")
    else:
        print(f"‚ùå Aucun fichier audio n'a √©t√© trait√©.")

    return df



# FIRST FILTER : Hate speech detection in audio
def detect_hate_speech_in_audio(audio_path , include_intervals):
    ## TODO : Implement the hate speech detection in audio
    speech_ranges = include_intervals
    timestamps = []
    texts = []
    emotions = []
    # Speech_ranges_to_timestamps
    #timestamps, texts = Speech_ranges_to_timestamps(audio_path, speech_ranges)

    # audio_to_emotion 
    #emotions = Audio_to_emotion(audio_path,timestamps)

    df = pd.DataFrame({
        "timestamp": timestamps,
        "text": texts,
        "emotion": emotions
    })


    # PARTIE PREPROCESSE dataframe **********
    exploded_df = pd.concat([explode_row(row) for _, row in df.iterrows()], ignore_index=True)

    exploded_df["hate_snippet"] = exploded_df["hate_snippet"].apply(clean_hate_snippet)

    # Enlever les caract√®res sp√©ciaux
    exploded_df["text"] = exploded_df["text"].apply(clean_text_light)


    # Cr√©er une cl√© de groupe si l‚Äô√©motion ou le hate_snippet change
    group_key = ((exploded_df['emotion'] != exploded_df['emotion'].shift()) | (exploded_df['hate_snippet'] != exploded_df['hate_snippet'].shift())).cumsum()
    exploded_df = pd.concat([merge_consecutive(g) for _, g in exploded_df.groupby(group_key)], ignore_index=True)

    # Supprimer les lignes avec moins de 3 mots dans la colonne 'text'
    exploded_df = exploded_df[exploded_df["text"].str.split().str.len() >= 3]
    exploded_df['text'] = exploded_df['text'].str.strip()
    # Supprimer les lignes doublon
    exploded_df = exploded_df.drop_duplicates(subset='text')

    # Creer une nouvelle colonne "Label_hate" avec la fonction get_label_hate
    exploded_df["Label_hate"] = exploded_df.apply(lambda row: get_label_hate(row["timestamp"], row["hate_snippet"]), axis=1)
    # ************************

    df = EmoHateBert_prediction_from_csv(exploded_df, "EmoHateBert_local.pt", device="cuda")

    hate_speech_time_audio = [timestamp for timestamp, text, emotion, label in zip(df["timestamp"], df["text"], df["emotion"], df["predicted_label"]) if label == 1]
    return hate_speech_time_audio


def merge_consecutive(group):
    merged = []
    current_start = group['timestamp'].iloc[0][0]
    current_end = group['timestamp'].iloc[0][1]
    current_text = group['text'].iloc[0]

    for i in range(1, len(group)):
        prev_end = group['timestamp'].iloc[i - 1][1]
        curr_start, curr_end_val = group['timestamp'].iloc[i]

        if prev_end == curr_start:
            current_end = curr_end_val
            current_text += ' ' + group['text'].iloc[i]
        else:
            merged.append({
                'timestamp': f"{current_start} - {current_end}",
                'text': current_text,
                'emotion': group['emotion'].iloc[i - 1],
                'hate_snippet': group['hate_snippet'].iloc[i - 1]
            })
            current_start = curr_start
            current_end = curr_end_val
            current_text = group['text'].iloc[i]

    merged.append({
        'timestamp': f"{current_start} - {current_end}",
        'text': current_text,
        'emotion': group['emotion'].iloc[-1],
        'hate_snippet': group['hate_snippet'].iloc[-1]
    })

    return pd.DataFrame(merged)


def clean_text_light(text):
    # Supprime les caract√®res tr√®s sp√©ciaux, mais garde les lettres, chiffres et ponctuation classique
    return re.sub(r"[^\w\s.,!?'-]", "", text)

def get_label_hate(timestamp, snippets):
    t_start, t_end = map(time_to_seconds, timestamp)
    label = 0
    if snippets is None:
        return 0
    for snippet in snippets:
        s_start, s_end = map(time_to_seconds, snippet)
        if t_start >= s_start and t_end <= s_end:
            return 1  # enti√®rement inclus
        elif t_start < s_end and t_end > s_start:
            label = 2  # partiellement inclus
    return label


def explode_row(row):
    timestamps = eval(row['Timestamps'])
    texts = eval(row['Texts'])
    emotions = eval(row['emotion'])
    hate_snippet = eval(row['hate_snippet']) if pd.notna(row['hate_snippet']) else [None] * len(timestamps)

    return pd.DataFrame({
        "hate_snippet": [hate_snippet] * len(timestamps),
        "timestamp": timestamps,
        "text": texts,
        "emotion": emotions
    })

def clean_hate_snippet(snippet):
    if isinstance(snippet, list) and snippet and snippet[0] is None:
        return None
    return snippet

from torch.utils.data import DataLoader, Dataset

class HateSpeechDataset(Dataset):
    def __init__(self, texts, emotions, labels, tokenizer, emotion2id):
        self.texts = texts
        self.emotions = emotions
        self.labels = labels
        self.tokenizer = tokenizer
        self.emotion2id = emotion2id

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        emotion = self.emotions[idx]
        label = self.labels[idx]

        tokens = self.tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')

        return {
            'input_ids': tokens['input_ids'].squeeze(0),
            'attention_mask': tokens['attention_mask'].squeeze(0),
            'emotion_id': torch.tensor(self.emotion2id[emotion]),
            'label': torch.tensor(label)
        }

class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.ce = nn.CrossEntropyLoss(reduction='none')  # important pour Focal

    def forward(self, logits, targets):
        ce_loss = self.ce(logits, targets)
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        return focal_loss.mean()


class BertWithEmotion(nn.Module):
    def __init__(self, emotion_vocab_size=5, emotion_dim=16, num_labels=2,
                 class_weights=None, use_focal=False, focal_alpha=1, focal_gamma=2):
        super().__init__()
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        self.bert_hidden = self.bert.config.hidden_size
        self.emotion_embed = nn.Embedding(emotion_vocab_size, emotion_dim)
        self.dropout = nn.Dropout(0.3)
        self.classifier = nn.Linear(self.bert_hidden + emotion_dim, num_labels)

        if use_focal:
            self.criterion = FocalLoss(alpha=focal_alpha, gamma=focal_gamma)
        else:
            if class_weights is not None:
                self.criterion = nn.CrossEntropyLoss(weight=class_weights)
            else:
                self.criterion = nn.CrossEntropyLoss()

    def forward(self, input_ids, attention_mask, emotion_id, labels=None):
        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls_vector = bert_out.last_hidden_state[:, 0, :]
        emotion_vector = self.emotion_embed(emotion_id)

        fusion = torch.cat([cls_vector, emotion_vector], dim=1)
        fusion = self.dropout(fusion)
        logits = self.classifier(fusion)

        if labels is not None:
            loss = self.criterion(logits, labels)
            return loss, logits
        return logits

def EmoHateBert_prediction_from_csv(df, model_path, emotion2id = {'ANGRY': 0, 'DISGUSTED': 1, 'FEARFUL': 2, 'HAPPY': 3, 'NEUTRAL': 4, 'SAD': 5, 'SURPRISED': 6, 'UNKNOWN': 7}, device='cpu'):
    # Charger les donn√©es
    df = df[["text", "emotion", "Label_hate"]].dropna()
    df["emotion"] = df["emotion"].str.upper()
    df["Label_hate"] = df["Label_hate"].replace(2, 1)  # optionnel si binaire

    # Tokenizer
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

    # Charger dataset
    dataset = HateSpeechDataset(
    texts=df["text"].tolist(),
    emotions=df["emotion"].tolist(),
    labels=df["Label_hate"].tolist(),
    tokenizer=tokenizer,
    emotion2id=emotion2id
    )
    
    loader = DataLoader(dataset, batch_size=16)

    # Charger le mod√®le
    model = BertWithEmotion(emotion_vocab_size=len(emotion2id), emotion_dim=16, num_labels=2)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    model.to(device)

    # Pr√©dictions
    all_preds = []
    with torch.no_grad():
        for batch in loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            emotion_id = batch["emotion_id"].to(device)

            logits = model(input_ids, attention_mask, emotion_id)
            preds = torch.argmax(logits, dim=1)
            all_preds.extend(preds.cpu().numpy())

    df["predicted_label"] = all_preds
    return df


# SECOND FILTER : Hate Speech Detection CLIP (symbole, geste obscene ... etc)

# Charger le mod√®le CLIP et son processor
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Descriptions (prompts) de classes visuelles possibles

visual_labels = [
    # üü• Hateful (13 labels bien distincts)
    "symbol of racial hatred",
    "violence against minorities",
    "racist or extremist graffiti",
    "homophobic hate gesture",
    "nazi salute",
    "white supremacist protest",
    "burning of religious symbols",
    "aggressive crowd with hate signs",
    "physical attack during a livestream",
    "threatening gesture on webcam",
    "explicit insult written on a sign",
    "harassment of LGBTQ individuals",
    "extremist propaganda banner",

    # üüß Visually intense but not necessarily hateful (8)
    "chaotic concert crowd",
    "people shouting in a protest",
    "police intervention in a public place",
    "fire on a stage during performance",
    "public fight in the street",
    "crowd mosh pit",
    "person yelling in a video call",
    "people arguing loudly",

    # üü© Safe / Non-hateful (19)
    "group of friends taking selfies",
    "people dancing together",
    "family celebration",
    "peaceful protest",
    "friendly street interview",
    "musician playing at a concert",
    "teenagers laughing on a call",
    "people hugging",
    "family dinner at home",
    "children playing outside",
    "teacher explaining to students",
    "Snapchat selfie with filter",
    "artistic mural in the street",
    "volunteers helping each other",
    "public event with diverse people",
    "sports activity with teammates",
    "respectful online conversation",
    "people cheering at a show",
    "cultural dance performance"
]


def detect_visual_hate_clip(image_path):
    image = Image.open(image_path).convert("RGB")

    # Pr√©parer les entr√©es pour CLIP
    inputs = clip_processor(text=visual_labels, images=image, return_tensors="pt", padding=True)

    # Obtenir les similarit√©s image ‚Üî texte
    with torch.no_grad():
        outputs = clip_model(**inputs)
        logits_per_image = outputs.logits_per_image
        probs = logits_per_image.softmax(dim=1).squeeze()

    results = {label: float(probs[i]) for i, label in enumerate(visual_labels)}

    hateful_labels = visual_labels[:13]
    safe_labels = visual_labels[13:]

    hate_scores = [results[label] for label in hateful_labels]
    safe_scores = [results[label] for label in safe_labels]

    # Moyenne pond√©r√©e (plus stable que max)
    avg_hate = sum(hate_scores) / len(hate_scores)
    avg_safe = sum(safe_scores) / len(safe_scores)

    # Meilleur score absolu (pour justifier le label final)
    top_label = max(results, key=results.get)
    top_score = results[top_label]

    # Analyse : marge de diff√©rence
    delta = abs(avg_hate - avg_safe)

    # D√©finir le label selon logique avanc√©e
    if delta < 0.05 and top_score < 0.3:
        final_label = "Uncertain"
    elif avg_hate * 0.85 > avg_safe :
        final_label = "Hate"
    else:
        final_label = "Safe"

    return {
        "label": final_label,
        "confidence_gap": round(delta, 4),
        "top_label": top_label,
        "top_score": round(top_score, 4),
        "avg_hate_score": round(avg_hate, 4),
        "avg_safe_score": round(avg_safe, 4),
        "all_scores": results
    }


def detect_hate_speech_CLIP(
    video_path: str,
    sampling_time_froid: float,
    sampling_time_chaud: float,
    time_to_recover: float,
    merge_final_snippet_time: float,
    detect_visual_hate_clip=None,
    skip_intervals=None  
):
    if detect_visual_hate_clip is None:
        raise ValueError("You must provide a detect_visual_hate_clip function")

    if skip_intervals is None:
        skip_intervals = []

    def is_skipped(time_sec):
        for start_str, end_str in skip_intervals:
            start = sum(int(x) * 60 ** i for i, x in enumerate(reversed(start_str.split(":"))))
            end = sum(int(x) * 60 ** i for i, x in enumerate(reversed(end_str.split(":"))))
            if start <= time_sec <= end:
                return True
        return False

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError("Could not open video file")

    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps

    current_time = 0.0
    state = "froid"
    time_in_chaud = 0.0
    hate_timestamps = []

    while current_time < duration:
        if is_skipped(current_time):
            current_time += sampling_time_chaud if state == "chaud" else sampling_time_froid
            continue

        cap.set(cv2.CAP_PROP_POS_MSEC, current_time * 1000)
        ret, frame = cap.read()
        if not ret:
            break

        temp_image_path = "/tmp/temp_frame.jpg"
        cv2.imwrite(temp_image_path, frame)

        result = detect_visual_hate_clip(temp_image_path)
        os.remove(temp_image_path)

        if result.get("label") == "Hate":
            hate_timestamps.append(current_time)
            state = "chaud"
            time_in_chaud = 0.0
        elif state == "chaud":
            time_in_chaud += sampling_time_chaud
            if time_in_chaud >= time_to_recover:
                state = "froid"

        current_time += sampling_time_chaud if state == "chaud" else sampling_time_froid

    cap.release()

    # √âtendre et fusionner les intervalles
    intervals = [(max(0, t - merge_final_snippet_time), min(duration, t + merge_final_snippet_time)) for t in hate_timestamps]
    merged_intervals = []
    for start, end in sorted(intervals):
        if not merged_intervals or start > merged_intervals[-1][1]:
            merged_intervals.append([start, end])
        else:
            merged_intervals[-1][1] = max(merged_intervals[-1][1], end)

    formatted_intervals = [[seconds_to_hhmmss(start), seconds_to_hhmmss(end)] for start, end in merged_intervals]

    return formatted_intervals


# THIRD FILTER : Hate Speech Detection in text from image

def seconds_to_hhmmss(seconds):
    return str(timedelta(seconds=int(seconds)))

reader = easyocr.Reader(['en'])  # detects the language of the text
nlp_classifier = pipeline("text-classification", model="Hate-speech-CNERG/dehatebert-mono-english")

def detect_hate_speech_in_image(image_path):
    # üñºÔ∏è OCR
    text_blocks = reader.readtext(image_path, detail=0)
    full_text = " ".join(text_blocks).strip()

    if not full_text:
        return {
            "text": None,
            "hate_detected": False,
            "score": 0.0,
            "reason": "No text detected"
        }

    # üß† NLP (classification hate speech)
    prediction = nlp_classifier(full_text)[0]

    return {
        "text": full_text,
        "hate_detected": prediction['label'].lower() == 'hate',
        "score": float(prediction['score']),
        "reason": prediction['label']
    }

def detect_hate_speech_OCR(
    video_path: str,
    sampling_time_froid: float,
    sampling_time_chaud: float,
    time_to_recover: float,
    merge_final_snippet_time: float,
    detect_hate_speech_in_image=None,
    skip_intervals=None  # nouvelle option : intervalles √† ignorer
):
    if detect_hate_speech_in_image is None:
        raise ValueError("You must provide a detect_hate_speech_in_image function")

    if skip_intervals is None:
        skip_intervals = []

    def seconds_to_hhmmss(seconds):
        from datetime import timedelta
        return str(timedelta(seconds=int(seconds)))

    def is_skipped(time_sec):
        for start_str, end_str in skip_intervals:
            start = sum(int(x) * 60 ** i for i, x in enumerate(reversed(start_str.split(":"))))
            end = sum(int(x) * 60 ** i for i, x in enumerate(reversed(end_str.split(":"))))
            if start <= time_sec <= end:
                return True
        return False

    import cv2
    import os

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError("Could not open video file")

    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps

    current_time = 0.0
    state = "froid"
    time_in_chaud = 0.0
    hate_timestamps = []

    while current_time < duration:
        if is_skipped(current_time):
            current_time += sampling_time_chaud if state == "chaud" else sampling_time_froid
            continue

        cap.set(cv2.CAP_PROP_POS_MSEC, current_time * 1000)
        ret, frame = cap.read()
        if not ret:
            break

        temp_image_path = "/tmp/temp_frame.jpg"
        cv2.imwrite(temp_image_path, frame)

        result = detect_hate_speech_in_image(temp_image_path)
        os.remove(temp_image_path)

        if result.get("hate_detected", False):
            hate_timestamps.append(current_time)
            state = "chaud"
            time_in_chaud = 0.0
        elif state == "chaud":
            time_in_chaud += sampling_time_chaud
            if time_in_chaud >= time_to_recover:
                state = "froid"

        current_time += sampling_time_chaud if state == "chaud" else sampling_time_froid

    cap.release()

    # √âtendre et fusionner les intervalles
    intervals = [(max(0, t - merge_final_snippet_time), min(duration, t + merge_final_snippet_time)) for t in hate_timestamps]
    merged_intervals = []
    for start, end in sorted(intervals):
        if not merged_intervals or start > merged_intervals[-1][1]:
            merged_intervals.append([start, end])
        else:
            merged_intervals[-1][1] = max(merged_intervals[-1][1], end)

    formatted_intervals = [[seconds_to_hhmmss(start), seconds_to_hhmmss(end)] for start, end in merged_intervals]

    return formatted_intervals

# FINAL FUNCTION

def merge_all_snippet_groups(list_of_snippet_lists):
    all_segments = []

    # Aplatir et convertir en secondes
    for snippet_list in list_of_snippet_lists:
        for start, end in snippet_list:
            all_segments.append([time_to_seconds(start), time_to_seconds(end)])

    # Trier et fusionner
    all_segments.sort()
    merged = []
    for seg in all_segments:
        if not merged or seg[0] > merged[-1][1]:
            merged.append(seg)
        else:
            merged[-1][1] = max(merged[-1][1], seg[1])

    # Reformat en HH:MM:SS
    return [[format_time(start), format_time(end)] for start, end in merged]


def detectHateSpeechSmartFilter(Video_path, Co2_release = "low"):
    tracker = EmissionsTracker(log_level="error" , allow_multiple_runs=True)
    tracker.start()

    if Co2_release == "low":
        CRC = [2, 1, 20, 4]
        Clip = [10, 3, 10, 3]
    elif Co2_release == "medium":
        CRC = [1, 0.5, 30, 4]
        Clip = [5, 2, 15, 4]
    elif Co2_release == "high":
        CRC = [0.5, 0.25, 40, 4]
        Clip = [3, 1, 20, 4]

    
    # Extraction de l'audio
    extract_audio(Video_path, "Audio.wav")
    # Pr√©traitement de l'audio
    speech_ranges = preprocess_audio("Audio.wav", "Audio_cleaned.wav")
    # first filter : hate speech detection in audio
    os.remove("Audio.wav")
    hate_speech_time_audio = detect_hate_speech_in_audio("Audio_cleaned.wav", include_intervals = speech_ranges)
    os.remove("Audio_cleaned.wav")
    print("‚úÖ Filter 1 : Hate speech detection in audio done !")
    # second filter : hate speech detection CLIP (obscene gesture, symbol ... etc)
    hate_speech_time_CLIP = detect_hate_speech_CLIP(    
        video_path=Video_path,
        sampling_time_froid= Clip[0],
        sampling_time_chaud= Clip[1],
        time_to_recover= Clip[2],
        merge_final_snippet_time= Clip[3],
        detect_visual_hate_clip= detect_visual_hate_clip,
        skip_intervals=hate_speech_time_audio 
        )
    print("‚úÖ Filter 2 : Hate speech detection using text embedding done !")
    # third filter : hate speech detection in text from image
    hate_speech_time_image_text = detect_hate_speech_OCR(
        video_path=Video_path,
        sampling_time_froid= CRC[0],
        sampling_time_chaud= CRC[1],
        time_to_recover= CRC[2],
        merge_final_snippet_time= CRC[3],
        detect_hate_speech_in_image=detect_hate_speech_in_image,
        skip_intervals= merge_all_snippet_groups([hate_speech_time_CLIP, hate_speech_time_audio])
    )
    print("‚úÖ Filter 3 : Hate speech detection using text from image done !")
    hate_speech_time = merge_all_snippet_groups([hate_speech_time_audio, hate_speech_time_CLIP, hate_speech_time_image_text])
    print("‚úÖ All filters done !" , hate_speech_time , "Hate speech detected !" , "C02 emissions : " , tracker.stop())
    C02_emissions = tracker.stop()
    return hate_speech_time, C02_emissions
